{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train_colab.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1Vxj5Oqa_XQAQ6bqhBCsfBLVm_Cidxzln","authorship_tag":"ABX9TyMIg2PFw7rXPrJH8mnJajtV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"BcpHCv34RzK_","executionInfo":{"status":"ok","timestamp":1617021617976,"user_tz":-330,"elapsed":2059,"user":{"displayName":"Ayush Suhane","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghr6DOVqzslON58L-Ss6UQIxRgfg0Q77bLJSlPeDw=s64","userId":"16041326713818823680"}}},"source":["from IPython.display import clear_output"],"execution_count":91,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cguc4sfVG70b","executionInfo":{"status":"ok","timestamp":1617021622635,"user_tz":-330,"elapsed":6509,"user":{"displayName":"Ayush Suhane","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghr6DOVqzslON58L-Ss6UQIxRgfg0Q77bLJSlPeDw=s64","userId":"16041326713818823680"}}},"source":["!pip install transformers\n","clear_output()"],"execution_count":92,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yny4Ho3wGR14","executionInfo":{"status":"ok","timestamp":1617021622638,"user_tz":-330,"elapsed":6245,"user":{"displayName":"Ayush Suhane","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghr6DOVqzslON58L-Ss6UQIxRgfg0Q77bLJSlPeDw=s64","userId":"16041326713818823680"}}},"source":["import os\n","os.chdir(\"/content/drive/MyDrive/NLP4IF/nlp4if\")"],"execution_count":93,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZDDmapX6ESo9","executionInfo":{"status":"ok","timestamp":1617022854711,"user_tz":-330,"elapsed":1581,"user":{"displayName":"Ayush Suhane","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghr6DOVqzslON58L-Ss6UQIxRgfg0Q77bLJSlPeDw=s64","userId":"16041326713818823680"}},"outputId":"cc066106-3fb4-426e-8a50-3280108bbef7"},"source":["!cat utils/train_utils.py"],"execution_count":103,"outputs":[{"output_type":"stream","text":["import numpy as np\r\n","from tqdm import tqdm\r\n","from transformers import AdamW, get_linear_schedule_with_warmup\r\n","from utils.losses import *\r\n","\r\n","# Evaluate\r\n","def predict_labels(nmodel, test_dataloader, device):\r\n","    nmodel.eval()\r\n","    y_preds = []\r\n","    y_test = []\r\n","    for i, batch in enumerate(test_dataloader):\r\n","        batch = [r.to(device) for r in batch]\r\n","        sent_id, mask, labels = batch\r\n","        with torch.no_grad():        \r\n","            ypred = nmodel(sent_id, mask)\r\n","            ypred = np.vstack([np.argmax(i.cpu().numpy(), axis=1) for i in ypred]).T\r\n","            labels=labels.cpu().numpy()\r\n","            y_preds.append(ypred)\r\n","            y_test.append(labels)\r\n","    y_preds = np.vstack(y_preds)\r\n","    y_test = np.vstack(y_test)\r\n","    return y_preds, y_test\r\n","    \r\n","# scorer\r\n","def scorer(y_preds, y_test):\r\n","    '''\r\n","    Averaged classwise accuracy\r\n","    '''\r\n","    accs = np.sum((y_preds==y_test).astype(int), axis=0)/len(y_test)*100\r\n","    total_accs = np.average(accs)\r\n","    return accs, total_accs\r\n","\r\n","def evaluate(nmodel, test_dataloader, device):\r\n","    y_preds, y_test = predict_labels(nmodel, test_dataloader, device)\r\n","    classwise_acc, total_acc = scorer(y_preds, y_test)\r\n","    return classwise_acc, total_acc\r\n","\r\n","# Define training loop here #\r\n","def train(model, dataloader, val_dataloader, device, num_epochs, lr=1e-5, loss_type=\"classwise_sum\"):\r\n","    optimizer = AdamW(model.parameters(),lr = lr)\r\n","    model.train()\r\n","\r\n","    loss_fn = None\r\n","    if loss_type == \"classwise_sum\":\r\n","        loss_fn = classwise_sum\r\n","    else:\r\n","        print(\"Loss {} Not Defined\".format(loss_type))\r\n","        sys.exit(1)\r\n","\r\n","    for epoch in range(num_epochs):\r\n","        for i,batch in enumerate(dataloader):\r\n","            batch = [r.to(device) for r in batch]\r\n","            sent_id, mask, labels = batch\r\n","            model.zero_grad()\r\n","            preds = model(sent_id, mask)\r\n","            loss_val = loss_fn(preds, labels)\r\n","            optimizer.zero_grad()\r\n","            loss_val.backward()\r\n","            optimizer.step()\r\n","            \r\n","        print(\"Epoch : {}  Loss : {}\".format(epoch, loss_val.cpu()))\r\n","        if(epoch%10==9):\r\n","            classwise_acc, total_acc = evaluate(model, val_dataloader, device)\r\n","            print('Classwise accuracy: ', classwise_acc, 'Total accuracy: ', total_acc)\r\n","    return model\r\n","\r\n","def train_v2(nmodel, training_dataloader, validation_dataloader, device, epochs = 4, lr1=2e-5, lr2=1e-4, loss_type=\"classwise_sum\"):\r\n","    total_steps = len(training_dataloader) * epochs\r\n","    bert_params = nmodel.embeddings\r\n","    bert_named_params = ['embeddings.'+name_ for name_, param_ in bert_params.named_parameters()]\r\n","    model_named_params = [name_ for name_, param_ in nmodel.named_parameters()]\r\n","    other_named_params = [i for i in model_named_params if i not in bert_named_params]\r\n","    params = []\r\n","\r\n","    for name, param in nmodel.named_parameters():\r\n","        if name in other_named_params:\r\n","            params.append(param)\r\n","    \r\n","    optimizer1 = AdamW(bert_params.parameters(), lr=2e-5, eps = 1e-8)\r\n","    optimizer2 = AdamW(params, lr=1e-4, eps = 1e-8)\r\n","    scheduler1 = get_linear_schedule_with_warmup(optimizer1, \r\n","                                                num_warmup_steps = 0, # Default value in run_glue.py\r\n","                                                num_training_steps = total_steps)\r\n","    scheduler2 = get_linear_schedule_with_warmup(optimizer2, \r\n","                                                num_warmup_steps = 0, # Default value in run_glue.py\r\n","                                                num_training_steps = total_steps)\r\n","\r\n","    loss_fn = None\r\n","    if loss_type == \"classwise_sum\":\r\n","        loss_fn = classwise_sum\r\n","    else:\r\n","        print(\"Loss {} Not Defined\".format(loss_type))\r\n","        sys.exit(1)\r\n","\r\n","    for epoch_i in tqdm(range(0, epochs)):\r\n","        total_train_loss = 0\r\n","        nmodel.train()\r\n","        for step, batch in enumerate(training_dataloader):\r\n","            batch = [r.to(device) for r in batch]\r\n","            sent_id, mask, labels = batch\r\n","            model.zero_grad()\r\n","            ypreds = nmodel(sent_id, mask)\r\n","            loss = loss_fn(ypred, b_labels)\r\n","            if step%50==0:\r\n","                print('Loss = '+str(total_train_loss/(step+1.00)))\r\n","            total_train_loss += loss\r\n","            optimizer1.zero_grad()\r\n","            optimizer2.zero_grad()\r\n","            loss.backward()\r\n","            torch.nn.utils.clip_grad_norm_(bert_params.parameters(), 1.0)\r\n","            optimizer1.step()\r\n","            optimizer2.step()\r\n","            scheduler1.step()\r\n","            scheduler2.step()\r\n","\r\n","        print(f'Total Train Loss = {total_train_loss}')\r\n","        print('#############    Validation Set Stats')\r\n","        classwise_acc, total_acc = evaluate(nmodel, val_dataloader, device)\r\n","        print('Classwise accuracy: ', classwise_acc, 'Total accuracy: ', total_acc)        \r\n","\r\n","    return nmodel"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c8Yp38N3GmPr","executionInfo":{"status":"ok","timestamp":1617023212406,"user_tz":-330,"elapsed":46853,"user":{"displayName":"Ayush Suhane","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghr6DOVqzslON58L-Ss6UQIxRgfg0Q77bLJSlPeDw=s64","userId":"16041326713818823680"}},"outputId":"0b7e1e0b-0648-4304-99ee-9bbf131417e8"},"source":["!python bert_train.py"],"execution_count":112,"outputs":[{"output_type":"stream","text":["2021-03-29 13:06:07.810442: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n","cuda\n","Dropping rows that contain nan in Q6_label or Q7_label\n","Dropping rows that contain nan in Q6_label or Q7_label\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2074: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  0% 0/10 [00:00<?, ?it/s]Loss = 0.0\n","Total Train Loss = 73.71954345703125\n","#############    Validation Set Stats\n","Classwise accuracy:  [66.29213483 44.94382022 52.80898876 34.83146067 33.70786517 79.7752809\n"," 60.6741573 ] Total accuracy:  53.290529695024084\n"," 10% 1/10 [00:03<00:32,  3.59s/it]Loss = 0.0\n","Total Train Loss = 70.57061767578125\n","#############    Validation Set Stats\n","Classwise accuracy:  [66.29213483 57.30337079 52.80898876 35.95505618 40.4494382  79.7752809\n"," 61.79775281] Total accuracy:  56.340288924558585\n"," 20% 2/10 [00:07<00:28,  3.58s/it]Loss = 0.0\n","Total Train Loss = 65.8370132446289\n","#############    Validation Set Stats\n","Classwise accuracy:  [82.02247191 65.16853933 74.15730337 53.93258427 60.6741573  82.02247191\n"," 78.65168539] Total accuracy:  70.9470304975923\n"," 30% 3/10 [00:10<00:24,  3.57s/it]Loss = 0.0\n","Total Train Loss = 56.54707717895508\n","#############    Validation Set Stats\n","Classwise accuracy:  [82.02247191 67.41573034 75.28089888 58.42696629 60.6741573  82.02247191\n"," 78.65168539] Total accuracy:  72.07062600321028\n"," 40% 4/10 [00:14<00:21,  3.56s/it]Loss = 0.0\n","Total Train Loss = 47.84797286987305\n","#############    Validation Set Stats\n","Classwise accuracy:  [84.26966292 64.04494382 75.28089888 57.30337079 59.5505618  80.8988764\n"," 79.7752809 ] Total accuracy:  71.58908507223114\n"," 50% 5/10 [00:17<00:17,  3.56s/it]Loss = 0.0\n","Total Train Loss = 43.549560546875\n","#############    Validation Set Stats\n","Classwise accuracy:  [83.14606742 64.04494382 74.15730337 55.05617978 62.92134831 82.02247191\n"," 77.52808989] Total accuracy:  71.2680577849117\n"," 60% 6/10 [00:21<00:14,  3.57s/it]Loss = 0.0\n","Total Train Loss = 40.263362884521484\n","#############    Validation Set Stats\n","Classwise accuracy:  [85.39325843 65.16853933 74.15730337 58.42696629 66.29213483 82.02247191\n"," 76.40449438] Total accuracy:  72.5521669341894\n"," 70% 7/10 [00:24<00:10,  3.57s/it]Loss = 0.0\n","Total Train Loss = 38.624168395996094\n","#############    Validation Set Stats\n","Classwise accuracy:  [85.39325843 71.91011236 77.52808989 62.92134831 67.41573034 80.8988764\n"," 76.40449438] Total accuracy:  74.63884430176564\n"," 80% 8/10 [00:28<00:07,  3.58s/it]Loss = 0.0\n","Total Train Loss = 35.84613800048828\n","#############    Validation Set Stats\n","Classwise accuracy:  [83.14606742 62.92134831 73.03370787 59.5505618  62.92134831 80.8988764\n"," 74.15730337] Total accuracy:  70.9470304975923\n"," 90% 9/10 [00:32<00:03,  3.59s/it]Loss = 0.0\n","Total Train Loss = 36.44507598876953\n","#############    Validation Set Stats\n","Classwise accuracy:  [82.02247191 59.5505618  73.03370787 58.42696629 65.16853933 80.8988764\n"," 74.15730337] Total accuracy:  70.46548956661316\n","100% 10/10 [00:35<00:00,  3.58s/it]\n","Classwise accuracy:  [84.90566038 71.69811321 75.47169811 69.81132075 60.37735849 88.67924528\n"," 69.81132075] \n","Total accuracy:  74.39353099730458\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"H5BXXmOJACRR","executionInfo":{"status":"ok","timestamp":1617023037845,"user_tz":-330,"elapsed":986,"user":{"displayName":"Ayush Suhane","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghr6DOVqzslON58L-Ss6UQIxRgfg0Q77bLJSlPeDw=s64","userId":"16041326713818823680"}}},"source":[""],"execution_count":108,"outputs":[]}]}